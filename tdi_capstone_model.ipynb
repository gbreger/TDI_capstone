{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53396a8b",
   "metadata": {},
   "source": [
    "This notebook contains the ML model that predicts log of the sales. It could just as well predict the sales themselves, but given the range of values for sales, it makes more sense to predict the log. An error of \\\\$50,000 on a 1.5 million dollars game would be less substantial than the same error on a \\\\$100,000 game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b3ee",
   "metadata": {},
   "source": [
    "The notebook assumes that the data set is already cleaned and ready to be used. See the data engineering notebook for the data preparation.\n",
    "\n",
    "There are three kinds of columns in the data set: numerical (e.g. sales), categorical (e.g. platform, genre), and free-form text (e.g. summary). The numerical data does not require any further preparation. The categorical data will be one-hot encoded. Some categorical columns contain singular values and will be transformed using OneHotEncoder. Other categorical columns contain lists that will use DictVectorizer. Note that the majority of the categorical columns contain numerical values, but these simply represent different classes of whatever data the columns hold. The free-form text will be transformed using TF-IDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c697d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from tdi_capstone_common_functions import *\n",
    "# the two functions imported are: standardize_string and pseudo_list_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928488c2",
   "metadata": {},
   "source": [
    "The data, loaded into a pandas data frame, has a lot of columns. While the data engineering notebook dropped columns that were not useful for predictions, specifically a lot of meta-data (e.g., the url for the game on IGDB), there are still columns that may be useful, but may as well not be. They were retained up to this point, but are now dropped. They may be used in future versions of the model. Some, however, can and shouled be dropped regardless, such asname, release_year, closest_match, match_score, id, first_release_date, and release_dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5680923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "drop_columns = [\n",
    "    'name',\n",
    "    'release_year',\n",
    "    'closest_match',\n",
    "    'match_score',\n",
    "    'id',\n",
    "    'first_release_date',\n",
    "    'release_dates',\n",
    "    'alternative_names',\n",
    "    'external_games',\n",
    "    'similar_games',\n",
    "    'language_supports',\n",
    "    'status',\n",
    "    'bundles',\n",
    "    'collections',\n",
    "    'parent_game',\n",
    "    'collection'\n",
    "    ]\n",
    "\n",
    "df = (pd.read_csv('data_complete.csv', index_col='index')\n",
    "      .drop(drop_columns, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98c323",
   "metadata": {},
   "source": [
    "Since different kinds of columns will be transformed using different transformers, it is easy to create lists of columns according to their kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4edf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with numerical data\n",
    "numeric_columns = ['sales_na', 'sales_eu', 'sales_jp', 'sales_other', 'sales_global']\n",
    "\n",
    "# columns with free-form text\n",
    "text_columns = ['summary', 'storyline']\n",
    "\n",
    "# columns that contain lists (imported as strings that look like lists)\n",
    "list_columns = ['age_ratings', 'game_modes', 'genres', 'themes', 'involved_companies', 'keywords',\n",
    "               'multiplayer_modes', 'franchises', 'game_engines', 'player_perspectives', 'game_localizations']\n",
    "\n",
    "# parse the columns that contain pseudo-lists into lists and populate those with a non-list (NaN) with an empty list\n",
    "df[list_columns] = df[list_columns].applymap(lambda x: pseudo_list_parser(x)).applymap(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# since most columns won't use OneHotEncoder, it is easier to exclude columns from df.columns than to explicitly list them out\n",
    "non_ohe_columns = numeric_columns + text_columns + list_columns\n",
    "# creates a list with the columns that do not appear in any of the above lists\n",
    "ohe_columns = [column_name for column_name in df.columns if column_name not in non_ohe_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0746c",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ebcc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD # used for dimensionality reduction\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922aa8d1",
   "metadata": {},
   "source": [
    "As mentioned above, there are three kinds of columns: numerical, categorical, and free-form text. Numerical data does not require any transformation at this point. The categorical columns can be divided into two subtypes, columns with singular values that will be transformed using OneHotEncoder and columns with lists of values that will be transformed with DictVectorizer. Lastly, free-form text will use TfidfVectorizer. Each of these kinds of transformations will be stored as a list of transformers with their relevant columns, so they can all eventually be put together into a single list of transformers passed onto the ColumnTransformer of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5eddb7",
   "metadata": {},
   "source": [
    "First to be treated are the OneHotEncoder categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6dd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_transformers = [('categorical', OneHotEncoder(handle_unknown='ignore'), ohe_columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938d0e6",
   "metadata": {},
   "source": [
    "Then the categorical columns with lists of values. Each one of these lists is first encoded as a dictionary (using a custom class) and then vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1374c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "#         X_trans = pd.DataFrame()\n",
    "        \n",
    "#         for column in X.columns:\n",
    "#             column_trans = [{key: 1 for key in item} for item in X[column]]\n",
    "#             X_trans[column] = column_trans\n",
    "        \n",
    "#         return X_trans\n",
    "        return [{key: 1 for key in row} for row in X] #, name=X.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651fae8",
   "metadata": {},
   "source": [
    "Because the transformer first needs to encode and then vectorize the list, a pipeline is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc1bd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column_vectorizer = Pipeline([\n",
    "    ('dict_encoder', DictEncoder()),\n",
    "    ('dict_vectorizer', DictVectorizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d3512",
   "metadata": {},
   "source": [
    "Then a list is created, where every element is the tuple format of a transformer (name, transformer, column(s)) for every column. This way, it is possible to access specific transformers by its name via the named_steps method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ae72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column_transformers = [(f'list_vect_{column}', list_column_vectorizer, f'{column}') for column in list_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672de3d2",
   "metadata": {},
   "source": [
    "Lastly, the free-form text columns are treated, where the strings are first standardized and then vectorized using TF-IDF. Very rare words are excluded, represented by the min_df value. Additionally, a relatively low max_df is set to exclude words that appear in a lot of game descriptions, since we want to use the most impactful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c02816ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_column_preprocessing(series):\n",
    "    \"\"\"\n",
    "    Standardizes a series containing free-form strings.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    series : pandas.Series()\n",
    "        A series of strings to be standardized, retaining only alphanumeric characters,\n",
    "        changing East Asian characters into Latin ones, and removing symbols and parentheses.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        A series of standardized strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    return series.map(standardize_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab35f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS = STOP_WORDS.difference({'he','his','her','hers'}).union({'ll', 've'})\n",
    "\n",
    "text_column_vectorizer = Pipeline([\n",
    "    ('standardize_text', FunctionTransformer(text_column_preprocessing)),\n",
    "    ('tfidf', TfidfVectorizer(min_df=20, max_df=0.5, stop_words=list(STOP_WORDS)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f75d87",
   "metadata": {},
   "source": [
    "Much like in the case with the list columns, a list of tuples is created, each referring to an individual free-fore text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b77814e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column_transformers = [(f'text_{column}', text_column_vectorizer, f'{column}') for column in text_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d858d8",
   "metadata": {},
   "source": [
    "It is now possible to build the model. All the features will undergo their respective transformations (remember that the numerical columns do not need any at this point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de23d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ColumnTransformer(\n",
    "    transformers = list_column_transformers + ohe_transformers + text_column_transformers,\n",
    "    remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45cc60",
   "metadata": {},
   "source": [
    "The model was trained with several regressors alongside appropriate parameter grids. These include LinearRegression, Ridge, RandomForestRegressor, and KNeighborsRegressor. Out of all of them, Ridge seemed to performed the best. In order to streamline building different versions of the model, a function allows for a quick customization of the regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a59134d",
   "metadata": {},
   "source": [
    "Note that the data set ends up having quite a lot of features, particularly due to the free-form text vectorizer. Given the number of observations, this needs to be taken into account. To balance that, some kind of dimensionality reduction can help to improve the model. Post-transformation, the data is stored in a sparse matrix, and since PCA doesn't work with sparse data, we'll use TruncatedSVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c3a5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regressor(requested_model='ridge', hyperparameters=None, requested_dr='default', cv=5, n_jobs=2):\n",
    "    \"\"\"\n",
    "    Creates a regressor with grid search cross-validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    requested_model : string or sklearn model, default 'ridge'\n",
    "        The name of the model as a string or an instantiated model\n",
    "        object itself. Currently supported models: LinearRegression, Ridge,\n",
    "        RandomForestRegressor, KNeighborsRegressor. Defaults to Ridge\n",
    "        if anything else is given.\n",
    "    hyperparameters : dict, default None\n",
    "        The hyperparameters for grid search cross-validation based on\n",
    "        the regressor. If not passed, defaults to the built-in ones.\n",
    "    requested_dr : string or list, default 'default'\n",
    "        Indicates how dimensionality reduction using TruncatedSVD should\n",
    "        work. Accepted strings are 'default' for the default values based\n",
    "        on the model chosen, 'aggressive' to reduce to hundreds of features,\n",
    "        or 'relaxed' to reduce to tens of features. Alternatively, a list\n",
    "        can be passed with the possible values.\n",
    "    cv : int, default 5\n",
    "        The number of folds in the cross validation.\n",
    "    n_jobs : int, default 2\n",
    "        The number of parallel jobs for the cross validation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    GridSearchCV object\n",
    "        An object based on the requested model or the default if that\n",
    "        type of regressor is not supported by the function. The parameter\n",
    "        grid is also set for the specified model.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The currently supported models are linear regression, ridge, random\n",
    "    forest, and k-nearest neighbors. All but the last one use a relaxed\n",
    "    dimensionality reduction (n_components = [100, 250, 500]) by default,\n",
    "    with k-nearest neighbors using an aggressive one\n",
    "    (n_components = [10, 20, 30]). The built-in default hyperparameters\n",
    "    for each are as follows:\n",
    "        Ridge :\n",
    "            'alpha': [0.1, 1.0, 10.0]\n",
    "        Random Forest :\n",
    "            'n_estimators': [10, 50, 100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 50],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        K-Nearest Neighbors :\n",
    "            'n_neighbors': [3, 5, 8, 10, 15]  \n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    if isinstance(requested_model, str):\n",
    "        supported_models = {\n",
    "            'linearregression': LinearRegression(),\n",
    "            'lr': LinearRegression(),\n",
    "            'linear': LinearRegression(),\n",
    "            'ridge': Ridge(),\n",
    "            'randomforestregressor': RandomForestRegressor(),\n",
    "            'randomforest': RandomForestRegressor(),\n",
    "            'kneighborsregressor': KNeighborsRegressor(),\n",
    "            'knn': KNeighborsRegressor()\n",
    "        }\n",
    "        \n",
    "        model = supported_models.get(requested_model.lower(), Ridge())\n",
    "        \n",
    "    # The default values for dimensionality reduction\n",
    "    dr_values = {\n",
    "        'relaxed': [100, 250, 500],\n",
    "        'aggressive': [10, 20, 30] # for, e.g., KNN\n",
    "    }\n",
    "    \n",
    "    # if the function was passed a specific list of dimensionality reduction values, it will use that list\n",
    "    if isinstance(requested_dr, list):\n",
    "        dr = requested_dr\n",
    "    else:\n",
    "        # if a string was passed, it will use one of the two built-in options, either aggressive or relaxed dimensionality\n",
    "        # reduction; or if the string was 'default' or another non-supported string, it will use a default set of values.\n",
    "        # These are based on the model requested, and so a function is used to determine which of the two built-in options\n",
    "        # to use.\n",
    "        def find_default_dr(model_for_dr):\n",
    "            aggressive_dr_models = [KNeighborsRegressor] # list of models which default to aggressive dimensionality reduction\n",
    "            \n",
    "            if model_for_dr in aggressive_dr_models:\n",
    "                return dr_values['aggressive']\n",
    "            \n",
    "            return dr_values['relaxed']\n",
    "        \n",
    "        # find the correct built-in value for dimensionality reduction\n",
    "        dr = dr_values.get(requested_dr, find_default_dr(model))\n",
    "\n",
    "    # Initiliazes the dimensionality reduction object\n",
    "    svd = TruncatedSVD()\n",
    "    \n",
    "    # Initializes the parameter grid with the dimensionality reduction values\n",
    "    param_grid = {'dim_reduction__n_components': dr}\n",
    "    \n",
    "    # If no parameter grid was passed, use the built-in values\n",
    "    if not hyperparameters:\n",
    "        \n",
    "        if isinstance(model, Ridge):\n",
    "            hyperparameters = {\n",
    "                'alpha': [0.1, 1.0, 10.0]\n",
    "            }\n",
    "        elif isinstance(model, RandomForestRegressor):\n",
    "            hyperparameters = {\n",
    "                'n_estimators': [10, 50, 100, 200, 300],\n",
    "                'max_depth': [None, 10, 20, 50],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        elif isinstance(model, KNeighborsRegressor):\n",
    "            hyperparameters = {\n",
    "                'n_neighbors': [3, 5, 8, 10, 15]   \n",
    "            }\n",
    "        else: # default if model does not have hyperparameters, e.g. linear regression\n",
    "            hyperparameters = {}\n",
    "    \n",
    "    # renames the keys so they can be properly accessed\n",
    "    hyperparameters = {f'regressor__{k}' : v for k, v in hyperparameters.items()}\n",
    "        \n",
    "    # Updates the parameter grid (currently only with dimensionality reduction) with the regressor's hyperparameters.\n",
    "    param_grid.update(hyperparameters)\n",
    "        \n",
    "    estimator = Pipeline([\n",
    "        ('dim_reduction', svd),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93038b",
   "metadata": {},
   "source": [
    "It is then possible to build the model with different kinds of regression algorithms or parameter grids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "839ba21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('features', features),\n",
    "    ('main_regressor', generate_regressor('ridge', cv=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bdb88a",
   "metadata": {},
   "source": [
    "Alternatively, we can generate several models, each with a different regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c0b402b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithms = ['lr', 'ridge', 'randomforest', 'knn']\n",
    "\n",
    "# models = dict()\n",
    "\n",
    "# for algorithm in algorithms:\n",
    "#     models[algorithm] = Pipeline([\n",
    "#         ('features', features),\n",
    "#         ('main_regressor', generate_regressor(algorithm, cv=10, n_jobs=-1))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b7298",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a95e52",
   "metadata": {},
   "source": [
    "As mentioned above, the numerical columns are only the sales, which are also the (basis of the) labels, so these can be dropped from the feature matrix passed onto the model. The labels are going to be the log of the global sales. In the future, it would be possible to have the model predict localized sales (e.g. EU, US, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "343f0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(numeric_columns, axis=1)\n",
    "y = np.log(df['sales_global'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742e7b7",
   "metadata": {},
   "source": [
    "The data is then split into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3481eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c0502",
   "metadata": {},
   "source": [
    "The model is then fit with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "202fa09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for model in models.values():\n",
    "#     model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89966d",
   "metadata": {},
   "source": [
    "Predictions can then be made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19314c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa16f5b",
   "metadata": {},
   "source": [
    "It's then possible to observe some metrics on the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dca65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83ef1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.758511190059073\n",
      "Mean squared error: 0.9915518979153569\n",
      "R^2: 0.6024894304074304\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean absolute error: {metrics.mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean squared error: {metrics.mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R^2: {metrics.r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9129eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ff214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Mean absolute error: {metrics.mean_absolute_error(y_test, y_pred)}\")\n",
    "# print(f\"Mean squared error: {metrics.mean_squared_error(y_test, y_pred)}\")\n",
    "# print(f\"R^2: {metrics.r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b83903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a8734",
   "metadata": {},
   "source": [
    "## Pickling\n",
    "To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41ab2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting co-efficients from a gridsearchcv object\n",
    "co1 = model.named_steps.main_regressor.best_estimator_.named_steps.regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc416be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7b952ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ff7e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.pickle', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acf9662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2 = loaded_model.named_steps.main_regressor.best_estimator_.named_steps.regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "171aeab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co1==co2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124d3c3",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50820251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "from ipywidgets import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# df_housing = pd.read_csv('small_data/Housing_Train.csv')\n",
    "# with open('small_data/Housing_Train_dict.pkl', \"rb\") as f:\n",
    "#     features_dict = pickle.load(f)\n",
    "\n",
    "# X = df_housing.drop('SalePrice', axis=1)\n",
    "# y = df_housing['SalePrice']\n",
    "\n",
    "def housing_plot(X, y):\n",
    "    def plotter(column):\n",
    "        valid_rows = X[column].notna()\n",
    "        plt.plot(X.loc[valid_rows, column], y[valid_rows], '.', color='k')\n",
    "#         plt.xlabel(features_dict[column])\n",
    "        plt.ylabel('Global sales (mil $)')\n",
    "    \n",
    "    return plotter\n",
    "\n",
    "# dropdown_values = {f\"{k}: {features_dict[k]}\": k for k in sorted(X.columns)}\n",
    "# widgets.interact(housing_plot(X, y), column=dropdown_values);\n",
    "widgets.interact(housing_plot(X_train, y_train), column=X_train.columns);\n",
    "\n",
    "# dropdown_values = {f\"{k}: {features_dict[k]}\": k for k in sorted(X.columns)}\n",
    "# widgets.interact(housing_plot(X, y), column=dropdown_values);\n",
    "\n",
    "# NOTE: This doesn't currently work with the columns that have lists as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd894c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of actual versus predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_predicted_vs_actual(actual_values, predicted_values):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(actual_values, predicted_values, alpha=0.5, label='Predicted vs Actual')\n",
    "#     plt.plot([min(actual_values), max(actual_values)], [min(actual_values), max(actual_values)], color='red', linestyle='--', label='Perfect Prediction')\n",
    "#     plt.xlabel('Actual Log Sales')\n",
    "#     plt.ylabel('Predicted Log Sales')\n",
    "#     plt.title('Actual vs Predicted Log Sales')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "# plot_predicted_vs_actual(y_test, y_pred)\n",
    "# plot_predicted_vs_actual(np.exp(y_test), np.exp(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predicted_vs_actual_enhanced(actual_values, predicted_values, extras='log of sales'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=actual_values, y=predicted_values, alpha=0.5)\n",
    "    sns.lineplot(x=[min(actual_values), max(actual_values)], y=[min(actual_values), max(actual_values)], color='red', linestyle='--')\n",
    "    plt.xlabel(f'Actual values ({extras})')\n",
    "    plt.ylabel(f'Predicted values ({extras})')\n",
    "    plt.title(f'Actual vs predicted values ({extras})')\n",
    "    plt.show()\n",
    "\n",
    "plot_predicted_vs_actual_enhanced(y_test, y_pred)\n",
    "plot_predicted_vs_actual_enhanced(np.exp(y_test), np.exp(y_pred), 'mil $')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "actual_values = np.random.normal(loc=10, scale=2, size=100)\n",
    "predicted_values = actual_values + np.random.normal(loc=0, scale=1, size=100)\n",
    "\n",
    "# plot_predicted_vs_actual(actual_values, predicted_values)\n",
    "plot_predicted_vs_actual_enhanced(actual_values, predicted_values, 'mil $')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359dcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e98cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD()\n",
    "\n",
    "# regressor = Ridge()\n",
    "# #regressor = RandomForestRegressor()\n",
    "# # regressor = KNeighborsRegressor()\n",
    "\n",
    "# param_grid = {\n",
    "# # relaxed dimensionality reduction:\n",
    "#     'dim_reduction__n_components': [100, 250, 500],\n",
    "# # aggresive dimensionality reduction (for, e.g., KNN):\n",
    "# #     'dim_reduction__n_components': [10, 20, 30],\n",
    "\n",
    "# # Ridge hyperparameters\n",
    "#    'regressor__alpha': [0.1, 1.0, 10.0]\n",
    "# # KNN hyperparameters\n",
    "# #     'regressor__n_neighbors': [3, 5, 8, 10, 15]\n",
    "# # RandomForest hyperparameters:\n",
    "# #     'regressor__n_estimators': [10, 50, 100],#, 200, 300],\n",
    "# #     'regressor__max_depth': [None, 10, 30], # 20, 50],\n",
    "# #     'regressor__min_samples_split': [2, 5, 10],\n",
    "# #     'regressor__min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# estimator = Pipeline([\n",
    "#     ('dim_reduction', svd),\n",
    "#     ('regressor', regressor)\n",
    "# ])\n",
    "\n",
    "# gs = GridSearchCV(\n",
    "#     estimator,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,\n",
    "#     n_jobs=2\n",
    "# )\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('features', features),\n",
    "#     ('main_regressor', gs)\n",
    "# ])\n",
    "\n",
    "# pipe.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
