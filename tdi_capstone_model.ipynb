{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7096a0",
   "metadata": {},
   "source": [
    "This notebook contains the ML model that predicts log of the sales. It could just as well predict the sales themselves, but given the range of values for sales, it makes more sense to predict the log. An error of \\\\$50,000 on a 1.5 million dollars game would be less substantial than the same error on a \\\\$100,000 game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b3ee",
   "metadata": {},
   "source": [
    "The notebook assumes that the data set is already cleaned and ready to be used. See the data engineering notebook for the data preparation.\n",
    "\n",
    "There are three kinds of columns in the data set: numerical (e.g. sales), categorical (e.g. platform, genre), and free-form text (e.g. summary). The numerical data does not require any further preparation. The categorical data will be one-hot encoded. Some categorical columns contain singular values and will be transformed using OneHotEncoder. Other categorical columns contain lists that will use DictVectorizer. Note that the majority of the categorical columns contain numerical values, but these simply represent different classes of whatever data the columns hold. The free-form text will be transformed using TF-IDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c697d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from tdi_capstone_common_functions import *\n",
    "# the two functions imported are: standardize_string and pseudo_list_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928488c2",
   "metadata": {},
   "source": [
    "The data, loaded into a pandas data frame, has a lot of columns. While the data engineering notebook dropped columns that were not useful for predictions, specifically a lot of meta-data (e.g., the url for the game on IGDB), there are still columns that may be useful, but may as well not be. They were retained up to this point, but are now dropped. They may be used in future versions of the model. Some, however, can and shouled be dropped regardless, such asname, release_year, closest_match, match_score, id, first_release_date, and release_dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5680923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "drop_columns = [\n",
    "    'name',\n",
    "    'release_year',\n",
    "    'closest_match',\n",
    "    'match_score',\n",
    "    'id',\n",
    "    'first_release_date',\n",
    "    'release_dates',\n",
    "    'alternative_names',\n",
    "    'external_games',\n",
    "    'similar_games',\n",
    "    'language_supports',\n",
    "    'status',\n",
    "    'bundles',\n",
    "    'collections',\n",
    "    'parent_game',\n",
    "    'collection'\n",
    "    ]\n",
    "\n",
    "df = (pd.read_csv('data_complete.csv', index_col='index')\n",
    "      .drop(drop_columns, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955f65a",
   "metadata": {},
   "source": [
    "Since different kinds of columns will be transformed using different transformers, it is easy to create lists of columns according to their kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4edf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with numerical data\n",
    "numeric_columns = ['sales_na', 'sales_eu', 'sales_jp', 'sales_other', 'sales_global']\n",
    "\n",
    "# columns with free-form text\n",
    "text_columns = ['summary', 'storyline']\n",
    "\n",
    "# columns that contain lists (imported as strings that look like lists)\n",
    "list_columns = ['age_ratings', 'game_modes', 'genres', 'themes', 'involved_companies', 'keywords',\n",
    "               'multiplayer_modes', 'franchises', 'game_engines', 'player_perspectives', 'game_localizations']\n",
    "\n",
    "# parse the columns that contain pseudo-lists into lists and populate those with a non-list (NaN) with an empty list\n",
    "df[list_columns] = df[list_columns].applymap(lambda x: pseudo_list_parser(x)).applymap(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# since most columns won't use OneHotEncoder, it is easier to exclude columns from df.columns than to explicitly list them out\n",
    "non_ohe_columns = numeric_columns + text_columns + list_columns\n",
    "# creates a list with the columns that do not appear in any of the above lists\n",
    "ohe_columns = [column_name for column_name in df.columns if column_name not in non_ohe_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0746c",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ebcc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD # used for dimensionality reduction\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92fb94",
   "metadata": {},
   "source": [
    "As mentioned above, there are three kinds of columns: numerical, categorical, and free-form text. Numerical data does not require any transformation at this point. The categorical columns can be divided into two subtypes, columns with singular values that will be transformed using OneHotEncoder and columns with lists of values that will be transformed with DictVectorizer. Lastly, free-form text will use TfidfVectorizer. Each of these kinds of transformations will be stored as a list of transformers with their relevant columns, so they can all eventually be put together into a single list of transformers passed onto the ColumnTransformer of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e2307",
   "metadata": {},
   "source": [
    "First to be treated are the OneHotEncoder categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d6dd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_transformers = [('categorical', OneHotEncoder(handle_unknown='ignore'), ohe_columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0671b89",
   "metadata": {},
   "source": [
    "Then the categorical columns with lists of values. Each one of these lists is first encoded as a dictionary (using a custom class) and then vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1374c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "#         X_trans = pd.DataFrame()\n",
    "        \n",
    "#         for column in X.columns:\n",
    "#             column_trans = [{key: 1 for key in item} for item in X[column]]\n",
    "#             X_trans[column] = column_trans\n",
    "        \n",
    "#         return X_trans\n",
    "        return [{key: 1 for key in row} for row in X] #, name=X.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d3613",
   "metadata": {},
   "source": [
    "Because the transformer first needs to encode and then vectorize the list, a pipeline is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc1bd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column_vectorizer = Pipeline([\n",
    "    ('dict_encoder', DictEncoder()),\n",
    "    ('dict_vectorizer', DictVectorizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187b17b",
   "metadata": {},
   "source": [
    "Then a list is created, where every element is the tuple format of a transformer (name, transformer, column(s)) for every column. This way, it is possible to access specific transformers by its name via the named_steps method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7ae72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column_transformers = [(f'list_vect_{column}', list_column_vectorizer, f'{column}') for column in list_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc60d37",
   "metadata": {},
   "source": [
    "Lastly, the free-form text columns are treated, where the strings are first standardized and then vectorized using TF-IDF. Very rare words are excluded, represented by the min_df value. Additionally, a relatively low max_df is set to exclude words that appear in a lot of game descriptions, since we want to use the most impactful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c02816ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_column_preprocessing(series):\n",
    "    \"\"\"\n",
    "    Standardizes a series containing free-form strings.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    series : pandas.Series()\n",
    "        A series of strings to be standardized, retaining only alphanumeric characters,\n",
    "        changing East Asian characters into Latin ones, and removing symbols and parentheses.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        A series of standardized strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    return series.map(standardize_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4922be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS = STOP_WORDS.difference({'he','his','her','hers'}).union({'ll', 've'})\n",
    "\n",
    "text_column_vectorizer = Pipeline([\n",
    "    ('standardize_text', FunctionTransformer(text_column_preprocessing)),\n",
    "    ('tfidf', TfidfVectorizer(min_df=20, max_df=0.5, stop_words=list(STOP_WORDS)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8ea3e",
   "metadata": {},
   "source": [
    "Much like in the case with the list columns, a list of tuples is created, each referring to an individual free-fore text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b77814e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column_transformers = [(f'text_{column}', text_column_vectorizer, f'{column}') for column in text_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under construction: allows for a quick and easy creation of a regressor.\n",
    "# This will include the GS component and the DR component.\n",
    "\n",
    "# def generate_regressor(model='ridge', param_grid=None):\n",
    "#     \"\"\"\n",
    "#     Creates a regressor to be used by the model to allow for quick\n",
    "#     customization of the model.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     model : string or sklearn model, default 'ridge'\n",
    "#         The name of the model as a string or the model object itself.\n",
    "#         Currently supported models are LinearRegression, Ridge,\n",
    "#         RandomForestRegressor, KNeighborsRegressor. Defaults to Ridge\n",
    "#         if anything else is given, as Ridge is currently the most\n",
    "#         accurate regressor.\n",
    "#     param_grid : dict, default None\n",
    "#         The parameter grid for grid search cross-validation based on\n",
    "#         the regressor. If no param_grid is passed, defaults to the one\n",
    "#         hard-coded in the function.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     sklearn regressor\n",
    "#         The regressor object requested or the default if that type of\n",
    "#         regressor is not supported by the function.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if isinstance(model, str):\n",
    "#         model = model.lower()\n",
    "#         elif model == 'linearregression':\n",
    "#             model = LinearRegression()\n",
    "#         elif model == 'randomforestregressor':\n",
    "#             model = RandomForestRegressor()\n",
    "#         elif model == 'kneighborsregressor':\n",
    "#             model = KNeighborsRegressor()\n",
    "#         else:\n",
    "#             model = Ridge() # the default regressor\n",
    "    \n",
    "#     if isinstance(model, LinearRegression()):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc0f69",
   "metadata": {},
   "source": [
    "It is now possible to build the model. All the features will undergo their respective transformations (remember that the numerical columns do not need any at this point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cb3ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ColumnTransformer(\n",
    "    transformers = list_column_transformers + ohe_transformers + text_column_transformers,\n",
    "    remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993aa46",
   "metadata": {},
   "source": [
    "The data set will end up having quite a lot of features, primarily due to the free-form text vectorizer, especially given the number of observations. To balance that, some kind of dimensionality reduction can help to improve the model. Post-transformation, the data is stored in a sparse matrix, and PCA doesn't work with sparse data. Instead, we use TruncatedSVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b148c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86109480",
   "metadata": {},
   "source": [
    "Several regressors were tried with appropriate parameter grids. These include LinearRegression, Ridge, RandomForestRegressor, and KNeighborsRegressor. Out of all of them, Ridge seemed to performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e98cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressor = Ridge()\n",
    "regressor = RandomForestRegressor()\n",
    "# regressor = KNeighborsRegressor()\n",
    "\n",
    "param_grid = {\n",
    "# relaxed dimensionality reduction:\n",
    "    'dim_reduction__n_components': [100, 250, 500],\n",
    "# aggresive dimensionality reduction (for, e.g., KNN):\n",
    "#     'dim_reduction__n_components': [10, 20, 30],\n",
    "\n",
    "# Ridge hyperparameters\n",
    "#    'regressor__alpha': [0.1, 1.0, 10.0]\n",
    "# KNN hyperparameters\n",
    "#     'regressor__n_neighbors': [3, 5, 8, 10, 15]\n",
    "# RandomForest hyperparameters:\n",
    "    'regressor__n_estimators': [10, 50, 100, 200, 300],\n",
    "    'regressor__max_depth': [None, 10, 20, 50],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "estimator = Pipeline([\n",
    "    ('dim_reduction', svd),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('features', features),\n",
    "    ('main_regressor', gs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f7d1e",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998f548",
   "metadata": {},
   "source": [
    "As mentioned above, the numerical columns are only the sales, which are also the (basis of the) labels, so these can be dropped from the feature matrix passed onto the model. The labels are going to be the log of the global sales. In the future, it would be possible to have the model predict localized sales (e.g. EU, US, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "343f0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(numeric_columns, axis=1)\n",
    "y = np.log(df['sales_global'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7486e0f",
   "metadata": {},
   "source": [
    "The data is then split into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3481eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5e7b6",
   "metadata": {},
   "source": [
    "The model is then fit with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0bb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe.named_steps.main_regressor.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563274fb",
   "metadata": {},
   "source": [
    "Predictions can then be made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9129eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9ed9e",
   "metadata": {},
   "source": [
    "It's then possible to observe some metrics on the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ff214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(f\"Mean absolute error: {metrics.mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean squared error: {metrics.mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R^2: {metrics.r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b83903",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
