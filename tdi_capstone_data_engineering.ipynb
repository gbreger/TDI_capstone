{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52ede48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2def6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A distinct game entry is defined by the unique combination of the following parameters\n",
    "\n",
    "unique_game = ['name', 'platform', 'release_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427d7ab",
   "metadata": {},
   "source": [
    "The project merges two data sets for training the model and predicting sales. The first is a sales data set, retreived from Kaggle (https://www.kaggle.com/datasets/thedevastator/global-video-game-sales-and-reviews). It contains a few other features, such as genre, critic scores, etc. The other data set contains information about games and was generated by accessing IGDB, an online games database (www.igdb.com) via its API. Before merging, the data sets need to be processed and aligned in such a way that they can be matched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab94656",
   "metadata": {},
   "source": [
    "The sales data set is dealt with first as it does not require much in terms of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a1ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the sales data csv into a pandas dataframe\n",
    "\n",
    "# names of the dataframe columns\n",
    "sales_columns = ['index', 'name', 'platform', 'release_year', 'genre', 'publisher',\n",
    "                 'sales_na', 'sales_eu', 'sales_jp', 'sales_other', 'sales_global',\n",
    "                 'critic_score', 'critic_count', 'user_score', 'user_count', 'developer', 'rating']\n",
    "\n",
    "# names of the columns to be dropped\n",
    "sales_drop_columns = ['critic_score', 'critic_count', 'user_score', 'user_count']\n",
    "\n",
    "# reading csv file into dataframe\n",
    "df_sales = (pd.read_csv('sales_data.csv', skiprows=1, names=sales_columns, index_col='index')\n",
    "            .drop(sales_drop_columns, axis=1)\n",
    "            .drop_duplicates() # There are 209 duplicated rows, which are removed here\n",
    "            .dropna(subset=['name'])) # There are 2 rows that have NaN as 'name' and therefore cannot be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c54133",
   "metadata": {},
   "source": [
    "The release year column in this data set contains floats, all of which are either a year number followed by .0 or a NaN. We want years to be ints as well as handle the NaN values. Release year is one of the features that are used to distinguish one game from another, so normally, we would not be able to use rows containing NaN values in these features. However, it is possible to replace the NaN values with the correct values manually, so these rows can still be useful. It is possible to designate data types for specific columns with read_csv, but pandas cannot do this conversion because of the NaN values. Instead, the NaN values are filled in with -1, an int itself which doubles as a flag, so these rows can be corrected manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c390ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: immediately after this I would put in the code that manually corrected the missing values.\n",
    "# I want to keep the fillna(-1) just in case I missed or forgot to correct one of these rows.\n",
    "\n",
    "df_sales['release_year'] = df_sales['release_year'].fillna(-1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for deprication - df_games also uses it for now, so see if I can change it there as well\n",
    "\n",
    "def convert_types(value, new_type, flag=-1):\n",
    "    \"\"\"\n",
    "    Cast a variable from one data type to another if possible.\n",
    "    \n",
    "    This function requires a lambda function to be passed to the map method instead of directly mapping the function, i.e.:\n",
    "    map(lambda x: convert_types(x, int)) instead of simply map(convert_types).\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    value : object\n",
    "        The value to be recast.\n",
    "    new_type : data type\n",
    "        The new data type for recasting.\n",
    "    flag : object, default -1\n",
    "        The return value in case casting was unsuccessful due to ValueError, e.g. casting a np.nan to an int.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    object\n",
    "        The value as a new data type. If a ValueError occurs, returns the object passed onto the flag argument.\n",
    "    \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    This function solved issues for the columns 'user_score' (originally a string, that had 'tbd') and 'release_year'\n",
    "    (originally a float, that had NaN).\n",
    "    \n",
    "    .astype(dtype, errors='ignore') does almost the same, but returns the original value if unsuccessful instead of a flag.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return new_type(value)\n",
    "    except ValueError:\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b00d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the release year column into an int value\n",
    "# df_sales['release_year'] = df_sales['release_year'].map(lambda x: convert_types(x, int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e026d2",
   "metadata": {},
   "source": [
    "Now it is possible to filter the data set to focus on a specific range of years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f345f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data set to retain certain years, here 2010-2020 (inclusive)\n",
    "\n",
    "time_boundaries = {'start': 2010, 'end': 2020}\n",
    "\n",
    "def filter_by_time_boundaries(df, column, time_limits=time_boundaries, include_start=True, include_end=True, flag=False):\n",
    "    \"\"\"\n",
    "    Filters a dataframe to retain only rows that fall within a specified range of years.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame()\n",
    "        The dataframe object to be filtered.\n",
    "    column : str\n",
    "        The name of the column containing the years, according to which the function will evaluate which rows to keep.\n",
    "    time_limits : dict, default time_boundaries\n",
    "        A dictionary with two keys, 'start' and 'end', denoting the two end points of the range to retain. Values of this\n",
    "        dictionary are integers. The project's default values are 2010 and 2020, set in a variable called time_boundaries.\n",
    "    include_start : bool, default True\n",
    "        Whether the function is inclusive of the start year.\n",
    "    include_end : bool, default True\n",
    "        Whether the function is inclusive of the end year.\n",
    "    flag : object, default False\n",
    "        Whether the function is inclusive of years with values equal to the value of flag.\n",
    "        Note that these must be truthy values in order for the function to include these rows.\n",
    "        Therefore, the default of False would not include flagged rows.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The filtered dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # sets up the filter based on the start of the range\n",
    "    if include_start:\n",
    "        start_filter = (df[column] >= time_limits['start'])\n",
    "    else:\n",
    "        start_filter = (df[column] > time_limits['start'])\n",
    "    \n",
    "    # sets up the filter based on the end of the range\n",
    "    if include_end:\n",
    "        end_filter = (df[column] <= time_limits['end'])\n",
    "    else:\n",
    "        end_filter = (df[column] < time_limits['end'])\n",
    "    \n",
    "    # sets up the filter based on whether to include rows flagged for missing values\n",
    "    if flag:\n",
    "        flagged_filter = (df[column] == flag)\n",
    "        \n",
    "        # return rows if they fall between start and end (inclusive or exclusive) or if rows were flagged for missing values\n",
    "        return df[(start_filter & end_filter) | flagged_filter]\n",
    "    \n",
    "    # return only rows that fall between start and end (inclusive or exclusive), excluding flagged rows\n",
    "    return df[start_filter & end_filter]\n",
    "\n",
    "# Note: currently the project filters out games who did not have a release year (marked with a flag of -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3e8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the sales data set by years\n",
    "df_sales = filter_by_time_boundaries(df_sales, 'release_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f56f43",
   "metadata": {},
   "source": [
    "Some columns contain strings, which need to be standardized. Most of them are a simple case of lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2029d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with strings to lowercase\n",
    "regular_string_columns = ['platform', 'genre', 'publisher', 'developer', 'rating']\n",
    "\n",
    "# standardizes string columns with lowercase\n",
    "df_sales[regular_string_columns] = df_sales[regular_string_columns].applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b66fb1",
   "metadata": {},
   "source": [
    "The names of games are more complex. Names can include diacritics, non-alphabet characters, East Asian characters, symbols, etc. There are also not confined to a limited vocabulary, such as genre or platform. It is important to remember that the names of games are only important insofar as they helps identify unique entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45d34e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def standardize_string(string):\n",
    "    \"\"\"\n",
    "    Standardize strings by converting diacritics and East Asian characters\n",
    "    as well as removing symbols such as parentheses (and their inclused\n",
    "    content), retaining only alphanumeric characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    string : str\n",
    "        The string to be standardized.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The standardized string in lowercase and stripped of\n",
    "        preceding/following whitespace.\n",
    "    \"\"\"\n",
    "\n",
    "    # if string is not a str, return an empty string\n",
    "    if not isinstance(string, str):\n",
    "        return ''\n",
    "   \n",
    "    # convert everything to unicode, addressing diacritics as well as chinese characters\n",
    "    string = unidecode(string)\n",
    "    \n",
    "    # remove any non-alphanumeric character or non-space as well as parenthesis (and their enclosed content)\n",
    "    regex = r'\\([^)]*\\)|[^a-zA-Z0-9\\s]'\n",
    "    string = re.sub(regex, '', string)\n",
    "    \n",
    "    # standardize spacing to retain a single space between words\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    \n",
    "    # change to lowercase and strips whitespaces\n",
    "    return string.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "269b66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizes strings in the name column, handling non-alphanumeric characters as well as removing parentheses\n",
    "df_sales['name'] = df_sales['name'].map(lambda x: standardize_string(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ea69f",
   "metadata": {},
   "source": [
    "By grouping rows according to unique_game, I eliminate duplicates while saving the max value for other columns, which can be assumed to be more up-to-date (since sales can only increase, not decrease). For this data set, this affects only a single game (name = 'madden nfl 13'; platform = 'ps3'; release_year = 2012), which has two different sales values and therefore is not removed when duplicates are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c375dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = df_sales.groupby(unique_game).agg('max').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c2c52",
   "metadata": {},
   "source": [
    "With the sales data set ready, we now move to importing and preparing the games data set.\n",
    "\n",
    "The games data set contains a lot of columns, about half of which are useful or could potentially be useful. Most are self-explanatory.\n",
    "It's worth noting that the majority of columns do not contain text, but rather the ID representing this data. For example, an entry of an action game would have XXXXX under genres rather than 'action'. To decode these numbers, one has to access the respective endpoint via IGDB's API. Since the actual name of the genre is not important (and would simply be one-hot-encoded probably), there is no need to transform those, and we can simply work with those IDs.\n",
    "\n",
    "* name\n",
    "* alternative_names\n",
    "* first_release_date\n",
    "* release_dates: in-depth information on release dates based on region, platform, etc.\n",
    "* platforms\n",
    "* genres\n",
    "* themes\n",
    "* franchise\n",
    "* franchises: other franchises this entry belongs to\n",
    "* keywords: e.g., 'world war 2', 'steampunk'\n",
    "* game mode: e.g., single player, multiplayer\n",
    "* player_perspectives\n",
    "* multiplayer_modes\n",
    "* summary: text description of the game\n",
    "* storyline\n",
    "* parent_game\n",
    "* bundles: bundles containing this game\n",
    "* collections: collections featuring this entry\n",
    "* collection: the specific series the entry belongs to\n",
    "* language_supports\n",
    "* game_localizations\n",
    "* similar_games\n",
    "* involved_companies: note that this is under development at IGDB\n",
    "* game_engines: the type of engine the game uses\n",
    "* age_rating\n",
    "* category: 0 = main game, 1 = DLC, 2 = expansion; the majority, over 226k entries, are category 0\n",
    "* external_games: game platforms featuring this entry, e.g., Steam, GOG, Twitch, Epic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28479422",
   "metadata": {},
   "source": [
    "The rest of the columns in the games data set can be dropped with impunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db5179d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop in the games data set\n",
    "\n",
    "games_drop_columns = ['artworks', # images of the game\n",
    "                      'cover', # cover art\n",
    "                      'created_at', # date when game entry was created at IGDB\n",
    "                      'screenshots', # screenshots of the game\n",
    "                      'slug', # unique url name string\n",
    "                      'tags', # auto-generated numbers for complex API filtering\n",
    "                      'updated_at', # last time the entry was updated at IGDB\n",
    "                      'url', # link to the game's entry in IGDB\n",
    "                      'version_parent', # if the entry is a version of another entry, this is the ID of the parent entry\n",
    "                      'version_title', # title of this version, e.g. gold edition\n",
    "                      'checksum', # hash of the game entry\n",
    "                      'websites', # websites associated with entry, e.g., developer's game page\n",
    "                      'follows', # no. people following the game on IGDB (depricated)\n",
    "                      'videos', # videos of gameplay\n",
    "                      'hypes', # no. people following the game on IGDB before its release\n",
    "                      'dlcs', # ID of DLCs of the entry\n",
    "                      'expansions', # ID of expansions of the entry\n",
    "                      'remakes', # ID of remakes of the entry\n",
    "                      'expanded_games', # ID of expanded games related to this entry\n",
    "                      'remasters', #ID of games that are remastered versions of this entry\n",
    "                      'standalone_expansions', # ID of stand-alone expansions of this entry\n",
    "                      'aggregated_rating', # aggregated rating based on external critic scores\n",
    "                      'aggregated_rating_count', # no. external critic scores\n",
    "                      'rating', # rating based on public reviews on IGDB\n",
    "                      'rating_count', # no. public reviews on IGDB\n",
    "                      'total_rating', # average rating based on critic and public review scores\n",
    "                      'total_rating_count', # no. reviews overall\n",
    "                      'forks', #\n",
    "                      'ports' # the ports (other platforms) the entry has other than current\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563459e",
   "metadata": {},
   "source": [
    "Due to size restrictions on GitHub, the games data set is split into 10 files, so they need to be concatenated into a singular dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4269d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = pd.DataFrame()\n",
    "\n",
    "for i in range(0, 10):\n",
    "    df_partial = pd.read_csv(f'games_data_{i}.csv', low_memory=False, index_col='Unnamed: 0')\n",
    "    df_games = pd.concat([df_games, df_partial], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de4d3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the irrelevant columns, rename platforms to platform for consistency, and drop duplicates\n",
    "\n",
    "df_games = df_games.drop(games_drop_columns, axis=1).rename(columns={'platforms': 'platform'}).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582133b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the games dataset was stored as one file, this command would have done all of the above\n",
    "# df_games = (pd.read_csv('igdb_raw.csv', low_memory=False, index_col='Unnamed: 0')\n",
    "#             .drop(games_drop_columns, axis=1)\n",
    "#             .rename(columns={'platforms': 'platform'})\n",
    "#             .drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3da27",
   "metadata": {},
   "source": [
    "Since distinct games are identified by name, platform, and release year, we use the first_release_date to extract the release year. However, first_release_date is a floating point number that needs to be converted into a more manageable (and useful for us) format, namely a date format, from which we can then extract the release year itself (which is the same kind of data the sales data set has).\n",
    "Just as in the case with the sales data set, NaN values are replaced by -1 (as a flag value) and type cast into int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639211e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games['release_year'] = pd.to_datetime(df_games['first_release_date'], unit='s').dt.year.fillna(-1).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3438b7",
   "metadata": {},
   "source": [
    "Then the data set is filtered to retain only rows with games whose (initial) release year falls within the time boundaries set for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "597d16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = filter_by_time_boundaries(df_games, 'release_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eea082",
   "metadata": {},
   "source": [
    "Now that the games data set has all three features that make up a distinct entry properly formatted, any entry that does not have one of these three key features is removed. Unlike the case with the sales data set, which has slightly over 200 such enties and therefore can be handled manually, the games data set has over 95,000 such entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eef3f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows that have missing values in any of the fields that define a unique game (name, platform, year).\n",
    "# this can be done with the unique_game variable only after the creation of the 'release_year' column above.\n",
    "\n",
    "df_games = df_games.dropna(how='any', subset=unique_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d2d2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizes the name column and drops any row that returned an empty string (i.e. names with only special characters)\n",
    "\n",
    "df_games['name'] = df_games['name'].map(lambda x: standardize_string(x))\n",
    "df_games = df_games[df_games['name'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7d762",
   "metadata": {},
   "source": [
    "Quite a few of the columns of the games data set contain lists of values. These, however, are imported as strings, e.g. '[12623, 6231, 96023]'. Therefore, these strings need to be parsed into proper lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df25cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_list_parser(item, dtype=int, ignore_space=True):\n",
    "    \"\"\"\n",
    "    Parse a string that appears as a list into a list of elements of a specified data type.\n",
    "    It assumes elements are separated by a comma.\n",
    "    These kind of strings are a common outcome of reading lists from csv files into a dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    item : str\n",
    "        The string to be parsed.\n",
    "    dtype : data type, default int\n",
    "        The data type that would comprise the elements of the parsed list.\n",
    "    ignore_space : bool, default True\n",
    "        Whether to remove spaces (appearing most often after a comma in a list string) from the string before parsing.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list or object\n",
    "        Returns the parsed string as a list of elements of the specified data type.\n",
    "        If the first argument passed to the function is not a string (e.g. NaN),\n",
    "        returns the item without performing any operations.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(item, str):\n",
    "        if ignore_space:\n",
    "            item = item.replace(' ', '')\n",
    "        return [dtype(x) for x in item.replace('[','').replace(']', '').split(',')]\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For deprication\n",
    "\n",
    "def column_parser(column, dtype=int, ignore_space=True):\n",
    "    \"\"\"\n",
    "    Map the pseudo_list_parser onto a Series object, thereby parsing each value in the series into a list if possible.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    column : pandas.Series\n",
    "        The column with the values to be parsed.\n",
    "    dtype : data type, default int\n",
    "        The data type to be used for the list elements to be parsed from each value in the Series.\n",
    "    ignore_space : bool, default True\n",
    "        Whether to remove spaces (appearing most often after a comma in a list string) from the string before parsing.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        The mapped Series with each data point parsed into a list (if possible).\n",
    "    \"\"\"\n",
    "    \n",
    "    return column.map(lambda x: pseudo_list_parser(x, dtype, ignore_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a758c",
   "metadata": {},
   "source": [
    "In particular, the platform column requires special attention, since it is one of the three elements that define a distinct game entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "656f617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the platform column from a single string that looks like a list to an actual list of values.\n",
    "# each element of the list is a string itself, since that would be mapped to match the platform values in the\n",
    "# sales data set.\n",
    "\n",
    "#df_games['platform'] = column_parser(df_games['platform'], str)\n",
    "df_games['platform'] = df_games['platform'].map(lambda x: pseudo_list_parser(x, str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71767ad3",
   "metadata": {},
   "source": [
    "While both data sets have the platform column, they encode this information differently. For example, the sales data set has 'pc', while the games data set has '6' and '13' for Windows and DOS respectively. In order to identify the distinct games and merge the data sets correctly, the platform values of one data set must be mapped onto those of the other data set. The sales data set is less granular (e.g. it has a single 'pc' value for both Windows and DOS), yet does not impact the ability to identify distinct game entries. For that reason, the platform values of the sales data set will be mapped onto those of the games data set. For example, after the remapping, the games data set will have 'pc' for all cases where originally it had the values '6' and '13' as platforms.\n",
    "The following dictionary's keys are the platforms (IDs) that appear in the games data set, while its values are the corresponding platforms in the sales data set. This was accomplished by first querying the IGDB API's platforms endpoint and then looking up the names of the platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b5b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = values found in df_games['platform']; value = values found in df_sales['platform']\n",
    "games_to_sales_platform_dict = {\n",
    "    'atari 2600': '2600',\n",
    "    '37': '3ds',\n",
    "    '137': '3ds', # new 3ds\n",
    "    '20': 'ds', # nintendo ds\n",
    "    '159': 'ds', # nintendo dsi\n",
    "    '9': 'ps3',\n",
    "    '7': 'ps2',\n",
    "    '38': 'psp',\n",
    "    '6': 'pc', # windows\n",
    "    '13': 'pc', # DOS\n",
    "    '5': 'wii',\n",
    "    '12': 'x360',\n",
    "    '4': 'n64', # nintendo 64\n",
    "    '21': 'gc', # game cube\n",
    "    '11': 'xb', # xbox\n",
    "    '18': 'nes',\n",
    "    '24': 'gba', # game boy advance\n",
    "    '46': 'psv', # ps vita ; note also '165' = playstation vr, and '390' = playstation vr2 (both not included in this dict)\n",
    "    '48': 'ps4',\n",
    "    '49': 'xone', # xbox one\n",
    "    '19': 'snes', # super NES\n",
    "    '59': '2600', # atari 2600\n",
    "    '41': 'wiiu',\n",
    "    '32': 'sat', # sega saturn\n",
    "    '33': 'gb', # game boy\n",
    "    '22': 'gb', # game boy color\n",
    "    '136': 'ng', # neo geo ; there are other neo geo variations in df_platforms, but there is no relevant game between 2010-2020    \n",
    "    '29': 'gen', # sega genesis\n",
    "    '274': 'pcfx',\n",
    "    '23': 'dc', # dream cast\n",
    "    '50': '3do', # 3do interactive multiplayer\n",
    "    '57': 'ws', # wonderswan\n",
    "    '86': 'tg16', # turbografx-16/pc engine cd\n",
    "    '150': 'tg16', # turbografx-16/pc engine\n",
    "    '78': 'scd', # sega cd\n",
    "    '35': 'gg' # game gear\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbe042",
   "metadata": {},
   "source": [
    "To apply this to the entire platform column, we will write a function that will be mapped onto it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19336525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(search_key, value_mapping):\n",
    "    \"\"\"\n",
    "    Map one set of values to another, thereby matching values between two data sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_key : object or list\n",
    "        The singular value or list to be remapped.\n",
    "    value_mapping : dict\n",
    "        A dictionary, where keys are the values passed to the function in search_key and the values are the new values to\n",
    "        be used in the remapping.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list or object\n",
    "        If a list was passed as the first argument of the function, a list with the corresponding (remapped) values will\n",
    "        be returned. If no match for any value in search_key was found, and therefore no remapping could have taken place,\n",
    "        return an empty list. If search_key was a singular value, returns the corresponding (remapped) value if found, or\n",
    "        a np.nan otherwise.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The following code accomplishes this as well but provides less control:\n",
    "        for item in set(l).intersection(value_mapping):\n",
    "            remapped.values.append(value_mapping[item])\n",
    "    or alternatively:\n",
    "        remapped_values = [item for item in set(l).intersection(value_mapping)]\n",
    "    \n",
    "    Future development\n",
    "    ------------------\n",
    "    The following code would look in value_mapping to see if it finds a match. If it doesn't, it will keep the value as is,\n",
    "    and when a dataframe merge takes place, rows with these values will be dropped since there will be no match.\n",
    "    If no element in search_key appears in value_mapping, set the return value to np.nan. If there are elements, make sure\n",
    "    to remove any duplicate values by transforming the list into a set.\n",
    "        for item in search_key:\n",
    "            value = value_mapping.get(item, item)\n",
    "        remapped_values.append(value)\n",
    "        remapped_values = list(set(remapped_values))\n",
    "        if not remapped_values:\n",
    "            remapped_values = np.nan\n",
    "        else:\n",
    "            remapped_values = list(set(remapped_values))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handling the case in which search_key is a list\n",
    "    if isinstance(search_key, list):\n",
    "        \n",
    "        # initializes the return value as an empty list\n",
    "        remapped_values = []\n",
    "        \n",
    "        # For every item in the search_key list, the loop attempts to find it as a key in the value_mapping dict.\n",
    "        # If it cannot, it continues to the next iteration (due to the if statement).\n",
    "        # If it does, it appends the non-None value to remapped_values, which is the return value of the function\n",
    "        for item in search_key:\n",
    "            \n",
    "            # retrieving the new, remapped value or None if it was not found \n",
    "            value = value_mapping.get(item, None)\n",
    "            \n",
    "            # corresponding values are appended to the return value of the function\n",
    "            if value:\n",
    "                remapped_values.append(value)\n",
    "    \n",
    "    # Handling the case in which search_key was not a list (function assumes this means it is a single value)\n",
    "    else:\n",
    "        \n",
    "        # retrieves the new, remapped value or NaN if it was not found\n",
    "        remapped_values = value_mapping.get(search_key, np.nan)\n",
    "    \n",
    "    return remapped_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd35dfc",
   "metadata": {},
   "source": [
    "The dictionary of corresponding platforms across the two data sets is then fed to a function which maps one set of possible values onto another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32a100f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes the values in the platforms column to the values used for platform in the df_sales\n",
    "df_games['platform'] = df_games['platform'].map(lambda x: map_values(x, games_to_sales_platform_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40917e",
   "metadata": {},
   "source": [
    "# CONTINUE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61dcf9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform rows that have multiple platforms listed into separate rows for each platform, copying all other information\n",
    "\n",
    "df_games = df_games.explode('platform')\n",
    "\n",
    "# Any row that has NaN in the platform field is dropped and any row that has empty strings or lists for any of the\n",
    "# unique_game columns\n",
    "\n",
    "df_games = df_games.dropna(subset=['platform'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080d4a1",
   "metadata": {},
   "source": [
    "# END OF ORGANIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:::: I should import_api on forks endpoint to see what I get in return.\n",
    "# NOTE:::: I need to find what is the ID for 'action' in genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee279c5c",
   "metadata": {},
   "source": [
    "## games data import and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a87576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: MARKED FOR REMOVAL\n",
    "# # convert first_release_date to a datetime data type\n",
    "# df_games['first_release_date'] = pd.to_datetime(df_games['first_release_date'], unit='s')\n",
    "\n",
    "# # creates a new column ('release_year') that contains only the year of release\n",
    "# df_games['release_year'] = df_games['first_release_date'].dt.year.map(lambda x: convert_types(x, int))\n",
    "\n",
    "# # filters data by set time boundaries\n",
    "# df_games = filter_by_time_boundaries(df_games, 'release_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a2487a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts all columns except for specific ones\n",
    "\n",
    "columns_not_to_parse = ['id', 'name', 'summary', 'storyline', 'platform', 'release_year']\n",
    "\n",
    "columns_to_parse = list(df_games.columns)\n",
    "\n",
    "# removes columns_not_to_parse from columns_to_parse\n",
    "for column in columns_not_to_parse:\n",
    "    columns_to_parse.remove(column)\n",
    "\n",
    "# parse all relevant columns\n",
    "for column in columns_to_parse:\n",
    "#     df_games[column] = column_parser(df_games[column])\n",
    "    df_games[column] = df_games[column].map(lambda x: pseudo_list_parser(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mva(group):\n",
    "    \"\"\"\n",
    "    Aggregate multiple values, used for aggregating pandas dataframe groupby objects, into a single flattened list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group : pandas.Series\n",
    "        The series containing the different values that will be combined into the single flattened list to be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list or pandas.Series or np.nan\n",
    "        Returns a flattened list containing all the elements of the Series passed as an argument to the function, with\n",
    "        no duplicates. If this Series contain only a single element, no aggregation is necessary, and the function returns\n",
    "        the Series as is. If the Series is empty, there were only NaN values, and the function returns np.nan.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Pandas dataframe groupby objects' agg method passes every column of every group as a Series to the aggregate function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # removes all NaN values\n",
    "    group = group.dropna()\n",
    "    \n",
    "    # if the group is empty, then it means that there were only NaN values in it\n",
    "    if len(group) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # if the group has one element, it is the only one that needs to be returned\n",
    "    if len(group) == 1:\n",
    "        return group\n",
    "    \n",
    "    # otherwise, there are multiple elements that need to be combined into a list\n",
    "    aggregated_value = []\n",
    "    \n",
    "    for value in group:\n",
    "        if isinstance(value, list):\n",
    "            aggregated_value.extend(value)\n",
    "        else:\n",
    "            aggregated_value.append(value)\n",
    "        \n",
    "    return list(set(aggregated_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the behavior of how each column would be aggregated by using dictionaries, where key = column name, and\n",
    "# value = the function (e.g. mva) or the name of the function (e.g. 'min')\n",
    "\n",
    "# columns that have multiple values that need to be combined into a flattened list\n",
    "mva_columns = ['age_ratings', 'category', 'external_games', 'game_modes', 'genres', 'release_dates', 'similar_games',\n",
    "              'summary', 'themes', 'language_supports', 'involved_companies', 'keywords', 'multiplayer_modes', 'status',\n",
    "              'alternative_names', 'bundles', 'franchises', 'game_engines', 'player_perspectives', 'game_localizations',\n",
    "              'collections', 'parent_game', 'collection', 'storyline', 'franchise']\n",
    "mva_dict = {key: mva for key in mva_columns}\n",
    "\n",
    "# columns that can be aggregated by taking the minimum value\n",
    "min_columns = ['id', 'first_release_date']\n",
    "min_dict = {key: 'min' for key in min_columns}\n",
    "\n",
    "# creates a single dictionary with the above aggregation behavioral dictionaries.\n",
    "# This dictionary will be passed onto the agg method of the groupby object\n",
    "column_aggregation_dict = {**mva_dict, **min_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates the df_games in order to remove duplicate entries resulting from multiple entries in the dataset itself, likely\n",
    "# by people opening multiple entries for the same game on igdb.\n",
    "\n",
    "df_games_agg = df_games.groupby(unique_game).agg(column_aggregation_dict).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a08380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to comment out the time boundaries filter so I find the -1 year games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1329d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sales[df_sales['release_year'] == -1]['platform'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5202f",
   "metadata": {},
   "source": [
    "For the purpose of this project, games are defined as unique based on their name, release year, and platform. Thus, games that share a name but have different release years and/or platforms are considered different games since they can perform differently in terms of sales.\n",
    "\n",
    "In order to merge the two data sets (one containing the sales data and the other containing the game data), it is necessary to standardize and match the values in platforms between the two data sets. That is also true for the two other values that constitute a unique game, namely, game name and release year. Matching names is done further down this notebook and release years are simply integers and do not require any special treatment.\n",
    "\n",
    "The following function maps values of a given feature based on one data set onto the values of the same feature in the other data set. It is used to correlate platform values between the two data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ff7c4",
   "metadata": {},
   "source": [
    "Since names are one of the key features that mark a unique game (the others being release year and platform), it is important to standardize the strings that make up the names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afa630",
   "metadata": {},
   "source": [
    "## Sales data import and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512375c9",
   "metadata": {},
   "source": [
    "## Section dealing with rows flagged with missing years\n",
    "\n",
    "Platforms left to do: PS2, Wii, X360, DS, PS3, XB, 2600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sales[(df_sales['release_year'] == -1) & (df_sales['platform'] == '2600')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e367b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sales[df_sales['name'].str.contains('Super Robot Wars OG Saga: Masou Kishin II')]['name'][9739]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# platform = PC\n",
    "\n",
    "# inversion, 2012\n",
    "# Homeworld Remastered Collection, 2015\n",
    "# WRC: FIA World Rally Championship, 2010\n",
    "# GRID, 2019\n",
    "# Clockwork Empires, 2016\n",
    "# Dead Island: Riptide, 2013\n",
    "# Rocksmith, 2011\n",
    "# Test Drive Unlimited 2, 2011\n",
    "# Dead Space 3, 2013\n",
    "# LEGO Harry Potter: Years 5-7, 2011 | PC, 3DS, PSP\n",
    "# BioShock 2, 2010\n",
    "# Tomb Raider, 2013 \n",
    "# TERA, 2011\n",
    "# Call of Duty: Black Ops, 2010\n",
    "    \n",
    "# Disgaea 3: Absence of Detention, 2011 | PSV\n",
    "    \n",
    "# 3DS\n",
    "# Harvest Moon: The Tale of Two Towns, 2010\n",
    "# Pet Zombies, 2011\n",
    "# Face Racers: Photo Finish, 2011\n",
    "# The Hidden, 2011\n",
    "# Dream Trigger 3D, 2011\n",
    "# Beyond the Labyrinth, 2012\n",
    "\n",
    "# PSP\n",
    "# Danganronpa: Trigger Happy Havoc, 2010\n",
    "# Valkyria Chronicles III: Unrecorded Chronicles, 2011\n",
    "# Super Robot Wars OG Saga: Masou Kishin II - Revelation of Evil God, 2012\n",
    "# Fullmetal Alchemist: Brotherhood, 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45d610",
   "metadata": {},
   "source": [
    "## End of section for flagged rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93553039",
   "metadata": {},
   "source": [
    "## Mapping genres between games data and sales data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55035b37",
   "metadata": {},
   "source": [
    "### Exploration of data\n",
    "\n",
    "I used to following code to determine the correlations of the platforms between df_games and df_sales.\n",
    "I do not need to run any of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d67e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_platforms = pd.read_csv('platforms.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_platforms['name'] = df_platforms['name'].str.lower()\n",
    "#df_platforms['alternative_name'] = df_platforms['alternative_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d623272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_by_name(df, name):\n",
    "#     filt_alt_name = df['alternative_name'].str.contains(name)\n",
    "#     filt_name = df['name'].str.contains(name)\n",
    "#     filt_slug = df['slug'].str.contains(name)\n",
    "    \n",
    "#     filtered_df = df[filt_alt_name | filt_name | filt_slug]\n",
    "    \n",
    "#     return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c167085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter_by_name(df_platforms,'gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3fc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sales['platform'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e54ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sales[df_sales['platform'] == 'scd']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a09b11",
   "metadata": {},
   "source": [
    "## merging sales and games data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174783a4",
   "metadata": {},
   "source": [
    "Since a unique game is defined by the combination of its name, release year, and platform, it is necessary to make sure that all three of these can be matched. Release years and platforms are confined to a certain number of fixed values (e.g., 'pc' or 'ps4' for platforms and 2015 or 2019 for release years). These have already been dealt with above.\n",
    "\n",
    "Names of games, however, can wildly vary. Even the same game can have different spellings of its name (or could have been input differently, e.g., with colons and hyphens, or using digits vs. roman numerals). Instead of a one-to-one match like with release year and platform, names will be matched by closest match. This introduces a couple of complexities. First, the same name can be used by several games, often referring to older/newer releases of a title. So simply finding the closest match would not work, since this would result in finding the (first) closest match in the data set. This also reveals the second caveat here, which is that multiple names in one data set might match most closely to a single name in the other data set.\n",
    "\n",
    "These issues can be solved by making the assumption that in a given year and for a given platform, every game will have a different name. Put another way, every game for each year-platform combination will have a distinctively unique name. This allows to look for closest name matches within a given year-platform combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate_year_platform_dict(df, year_column='release_year', platform_column='platform'):\n",
    "    \"\"\"\n",
    "    Create a dictionary of all combinations of year-platform in the data set, where keys are release year and values are\n",
    "    a list of platforms for which games were made for that year (insofar as they appear in the data set).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The data frame containing the data set. Must contain columns with the names that appear in the two other arguments.\n",
    "    year_column : str, default 'release_year'\n",
    "        The name of the column containing the release year that will be used as the return value dictionary's keys.\n",
    "    platform_column : str, default 'platform'\n",
    "        The name of the column containig the platforms that will be used as the return value dictionary's values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with keys of release year and values of the platforms with games that were released that year.\n",
    "    \"\"\"\n",
    "    \n",
    "    return_dict = defaultdict(list)\n",
    "    \n",
    "    for year in df[year_column].unique():\n",
    "        for platform in df[df[year_column] == year][platform_column].unique():\n",
    "            return_dict[year].append(platform)\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5014c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all possible combination of year/platform found in the sales data\n",
    "\n",
    "sales_comb = generate_year_platform_dict(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d981426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all possible combination of year/platform found in the games data\n",
    "\n",
    "games_comb = generate_year_platform_dict(df_games_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary that contains of release year(key) and list of platforms (list) that are shared between the two datasets\n",
    "\n",
    "shared_comb = {}\n",
    "\n",
    "for sales_year, sales_platform in sales_comb.items():\n",
    "    \n",
    "    # retrieves the list of platforms from df_game according to the year from df_sales; or None if there were no platforms\n",
    "    # for that sales year\n",
    "    games_platform = games_comb.get(sales_year, None)\n",
    "    \n",
    "    # add to that sales year the platforms that are shared for both datasets for that particular year\n",
    "    if games_platform:\n",
    "        shared_comb[sales_year] = list(set(sales_platform).intersection(games_platform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_match(string, choices):\n",
    "    \"\"\"\n",
    "    Find the closest string from available choices based on edit distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    string : str\n",
    "        The string to be matched.\n",
    "    choices : iterable\n",
    "        The array of strings (most commonly a list or a Series) from which the closest match will be taken.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of three elements: the string that was the closest match, the match score of the closest match (0 = completely\n",
    "        different string, 100 = identical string), and the index within the iterable choices of the closest match.\n",
    "    \"\"\"\n",
    "    \n",
    "    match, score, index = process.extractOne(string, choices)\n",
    "    return match, score, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match_and_remove(string, choices):\n",
    "    \"\"\"\n",
    "    Find the closest string from available choices based on edit distance. Once a match is found, that particular choice is\n",
    "    removed from the avaiable choices to prevent multiple strings matching to the same choice.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    string : str\n",
    "        The string to be matched.\n",
    "    choices : iterable\n",
    "        The array of strings (most commonly a list or a Series) from which the closest match will be taken.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of four elements: the string that was the closest match, the match score of the closest match (0 = completely\n",
    "        different string, 100 = identical string), the index within the iterable choices of the closest match, and the array\n",
    "        of choices (after the removal of the choice that was matched).    \n",
    "    \"\"\"\n",
    "    \n",
    "    match, score, index = find_closest_match(string, choices)\n",
    "    choices = choices.drop(index)\n",
    "    return match, score, index, choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26debc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code goes over all year/platform combinations, one at a time, in the sales data and tries to find a match in the game data.\n",
    "# it slowly builds up a new dataframe that contains those matches, keeping the index in df_games to merge it later on\n",
    "\n",
    "df_sales_name_matched = pd.DataFrame()\n",
    "\n",
    "for year, platforms in shared_comb.items():\n",
    "    for platform in platforms:\n",
    "#        print(f'Checking {year} + {platform}')\n",
    "        filtered_sales = df_sales[(df_sales['release_year'] == year) & (df_sales['platform'] == platform)][unique_game]\n",
    "        filtered_games = df_games_agg[(df_games_agg['release_year'] == year) & (df_games_agg['platform'] == platform)][unique_game]\n",
    "        \n",
    "        choices = filtered_games['name']\n",
    "        filtered_sales['closest_match'], filtered_sales['match_score'], filtered_sales['index_in_df_games_agg'], choices = zip(*filtered_sales['name'].apply(lambda x: find_match_and_remove(x, choices)))\n",
    "        \n",
    "        df_sales_name_matched = pd.concat([df_sales_name_matched, filtered_sales], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb93390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match score of 90 seems to be the threshold where there are many good matches,\n",
    "# but anything below that results in many misses and only a few good matches.\n",
    "\n",
    "# then join in the columns from df_sales (after dropping the shared columns, i.e., unique_game) to the matched up rows\n",
    "df_sales_name_matched = df_sales_name_matched[df_sales_name_matched['match_score'] >= 90].join(df_sales.drop(unique_game, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges the sales data (also containing the matched indices from the games data) with the aggregated games data\n",
    "\n",
    "final_columns_to_drop = ['index_in_df_games_agg']\n",
    "\n",
    "df_final = (pd.merge(df_sales_name_matched,\n",
    "                     df_games_agg.drop(unique_game, axis=1), left_on='index_in_df_games_agg', right_index=True)\n",
    "            .drop(final_columns_to_drop, axis=1)\n",
    "            .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the complete data set into a CSV file.\n",
    "\n",
    "df_final.to_csv('data_complete.csv', index_label='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
