{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add8b3ee",
   "metadata": {},
   "source": [
    "## Outline\n",
    "This notebook contains the code for the ML model for the project.\n",
    "It assumes that the data it is given is already cleaned and ready for processing. The process of cleaning up the data is done in the data engineering notebook.\n",
    "\n",
    "It assumes that the dataset contains columns with three data types: numerical (e.g. sales), categorical (e.g., platform, genre), and descriptive (a text description of the game).\n",
    "\n",
    "The numerical data would not require any further manipulation.\n",
    "\n",
    "The categorical data will be one-hot encoded.\n",
    "\n",
    "The text description will need to be transformed using NLP methods. One way of doing this is some kind of count vectorizer, that will teach the model which words appear in better-selling games. Another way of thinking about it is to think of the text description as a \"review\" (like in yelp), and the corresponding sales figure as the \"rating.\" Can the model use the words in the text description to predict (or to use as part of other features to predict) the sales of a game?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f3b28",
   "metadata": {},
   "source": [
    "## Ideas about the model\n",
    "What does the model do?\n",
    "\n",
    "What does it need to do with the numerical, categorical, and descriptive data?\n",
    "\n",
    "This is a regression problem!\n",
    "\n",
    "Starting from the end:\n",
    "1. A regressor: what kind of regressor would work here? Given the data, I think I can try a linear model first. I can withhold a piece of the data and use it as a test set, to see how the linear model performs. If it doesn't do well (whatever that means), I can try other options. I need to remember that this is a regression problem and therefore should focus on regression estimators.\n",
    "2. The regressor will be fed the feature matrix.\n",
    "3. The feature matrix will include numerical data, categorical data, and descriptive (free-text) data.\n",
    "4. Does the numerical data need to be altered? The numerical data that I currently am thinking of is just the sales data, which is already scaled, so I don't think I need to do anything with the numerical data.\n",
    "5. Categorical data will need to be one-hot encoded. This is the majority of my features, from genre and platform, to franchise and potentially age-rating. So a OHE can be applied to all of these. This will increase the number of features greatly. Combined with the NLP data, this may result in a larger number of features than observations.\n",
    "6. The text data will need to be processed using NLP methods... HOW SO?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b02069",
   "metadata": {},
   "source": [
    "## Workflow of the model\n",
    "\n",
    "(0. Transforming the numerical data by scaling it - this step is not necessary because the only numerical data currently is the sales values, which are already scaled.)\n",
    "1. Transforming the categorical data - using OHE\n",
    "2. Transforming the text data - using NLP methods... TBD\n",
    "3. Training a regressor, starting out with LinearRegression\n",
    "4. Using CV\n",
    "\n",
    "Then:\n",
    "\n",
    "5. Test the model by splitting the data into a training and testing set and see how well it performs in predicting the sales of the withheld testing set.\n",
    "6. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c697d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdfe35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization of strings\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def standardize_string(string):\n",
    "\n",
    "    if not isinstance(string, str):\n",
    "        return ''\n",
    "        \n",
    "    # converts everything to unicode, addressing diacritics as well as chinese characters\n",
    "    string = unidecode(string)\n",
    "    \n",
    "    # removes any non-alphanumeric character or non-space as well as parenthesis (and their inclused content)\n",
    "    regex = r'\\([^)]*\\)|[^a-zA-Z0-9\\s]'\n",
    "    string = re.sub(regex, '', string)\n",
    "    \n",
    "    # standardizes spacing that there is only one space between each word\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    \n",
    "    # changes to lowercase and strips whitespaces\n",
    "    return string.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84e2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_list_parser(item, dtype=int, ignore_ws=True):\n",
    "\n",
    "    if isinstance(item, str):\n",
    "        if ignore_ws:\n",
    "            item = item.replace(' ', '')\n",
    "        return [dtype(x) for x in item.replace('[','').replace(']', '').split(',')]\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48447932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['summary'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132cc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[1701]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a425009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize_string('test$ing a random\\nblah')#df.iloc[1700]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c6d43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[1700]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a4ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# from gensim.utils import simple_preprocess\n",
    "# import re\n",
    "\n",
    "# def process_review(review):\n",
    "#     \"\"\"\n",
    "#     Splits review into sentences, then sentences into tokens. Returns \n",
    "#     nested list.\n",
    "#     \"\"\"\n",
    "#     words = [simple_preprocess(sentence, deacc=True) \n",
    "#              for sentence in re.split('\\.|\\?|\\!', review)\n",
    "#              if sentence]\n",
    "#     return words\n",
    "\n",
    "# # Flatten list to contain all sentences from all reviews\n",
    "\n",
    "# def flatten_text(text):\n",
    "    \n",
    "#     sentences = [sentence for review in text \n",
    "#                  for sentence in process_review(text)]\n",
    "    \n",
    "#     return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a87e9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_review('test$ing a random\\nblah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1b8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_review(df.iloc[1700]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928488c2",
   "metadata": {},
   "source": [
    "## Loading the data and parsing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5680923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "drop_columns = [\n",
    "    'name',\n",
    "    'release_year',\n",
    "    'closest_match',\n",
    "    'match_score',\n",
    "    'id',\n",
    "    'first_release_date',\n",
    "    'external_games',\n",
    "    'release_dates',\n",
    "    'similar_games',\n",
    "    'language_supports',\n",
    "    'status',\n",
    "    'alternative_names',\n",
    "    'bundles',\n",
    "    'collections',\n",
    "    'parent_game',\n",
    "    'collection'\n",
    "    ]\n",
    "\n",
    "df = (pd.read_csv('data_complete.csv', index_col='index')\n",
    "      .drop(drop_columns, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4edf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with numerical data\n",
    "numeric_columns = ['sales_na', 'sales_eu', 'sales_jp', 'sales_other', 'sales_global']\n",
    "\n",
    "# columns with text descriptions\n",
    "text_columns = ['summary', 'storyline']\n",
    "\n",
    "# columns that contain lists\n",
    "list_columns = ['age_ratings', 'game_modes', 'genres', 'themes', 'involved_companies', 'keywords',\n",
    "               'multiplayer_modes', 'franchises', 'game_engines', 'player_perspectives', 'game_localizations']\n",
    "# parse the columns that contain pseudo-lists into lists and populate those with non-list (=NaN) with empty lists\n",
    "df[list_columns] = df[list_columns].applymap(lambda x: pseudo_list_parser(x)).applymap(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# since most columns are categorical, it is simpler to exclude columns from df.columns than to explicitly list them out\n",
    "non_ohe_columns = numeric_columns + text_columns + list_columns\n",
    "\n",
    "ohe_columns = [column_name for column_name in df.columns if column_name not in non_ohe_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b660b1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4202, 25)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "465598bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_d = {}\n",
    "# store_d = {}\n",
    "\n",
    "# for item in df['summary']:\n",
    "    \n",
    "#     sum_words = standardize_string(item).split()\n",
    "#     for word in sum_words:\n",
    "#         sum_d[word] = sum_d.get(word, 0) + 1\n",
    "\n",
    "# for item in df['storyline']:\n",
    "    \n",
    "#     store_words = standardize_string(item).split()\n",
    "    \n",
    "#     for word in store_words:\n",
    "#         store_d[word] = store_d.get(word, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69547bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "\n",
    "# for k, v in store_d.items():\n",
    "#     if v > 100:\n",
    "#         count += 1\n",
    "        \n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0746c",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ebcc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01e6ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b5b7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1374c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "#         X_trans = pd.DataFrame()\n",
    "        \n",
    "#         for column in X.columns:\n",
    "#             column_trans = [{key: 1 for key in item} for item in X[column]]\n",
    "#             X_trans[column] = column_trans\n",
    "        \n",
    "#         return X_trans\n",
    "        return [{key: 1 for key in row} for row in X] #, name=X.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc1bd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column_vectorizer = Pipeline([\n",
    "    ('dict_encoder', DictEncoder()),\n",
    "    ('dict_vectorizer', DictVectorizer())\n",
    "#    (f'dv_{col}', DictVectorizer(), col) for col in \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7ae72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column_transformers = [(f'list_vect_{column}', list_column_vectorizer, f'{column}') for column in list_columns]\n",
    "#list_column_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d6dd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_transformers = [('categorical', OneHotEncoder(handle_unknown='ignore'), ohe_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c02816ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_column_preprocessing(series):\n",
    "    \n",
    "    return series.map(standardize_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b77814e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS = STOP_WORDS.difference({'he','his','her','hers'}).union({'ll', 've'})\n",
    "\n",
    "text_column_vectorizer = Pipeline([\n",
    "    ('text_parse_and_split', FunctionTransformer(text_column_preprocessing)),\n",
    "    ('tfidf', TfidfVectorizer(min_df=20, max_df=0.5, stop_words=list(STOP_WORDS)))\n",
    "])\n",
    "\n",
    "text_column_transformers = [(f'text_{column}', text_column_vectorizer, f'{column}') for column in text_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0e98cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ColumnTransformer(\n",
    "    transformers = list_column_transformers + ohe_transformers + text_column_transformers,\n",
    "    remainder='drop')\n",
    "\n",
    "svd = TruncatedSVD() # I think this is how to use it\n",
    "# pca doesn't work with sparse matrix\n",
    "\n",
    "# regressor = Ridge()\n",
    "regressor = RandomForestRegressor()\n",
    "# regressor = KNeighborsRegressor()\n",
    "\n",
    "param_grid = {\n",
    "# relaxed dimensionality reduction:\n",
    "    'dim_reduction__n_components': [10, 20, 30, 50, 100, 250, 500],\n",
    "# aggresive dimensionality reduction (for, e.g., KNN):\n",
    "#     'dim_reduction__n_components': [10, 20, 30],\n",
    "\n",
    "# Ridge hyperparameters\n",
    "#    'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "# KNN hyperparameters\n",
    "#     'regressor__n_neighbors': [3, 5, 8, 10, 15]\n",
    "# RandomForest hyperparameters:\n",
    "    'regressor__n_estimators': [10, 50, 100], #, 200, 300],\n",
    "    'regressor__max_depth': [None, 10, 20, 50],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "estimator = Pipeline([\n",
    "    ('dim_reduction', svd),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('features', features),\n",
    "    ('main_regressor', gs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "343f0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(numeric_columns, axis=1)\n",
    "y = np.log(df['sales_global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3481eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0bb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps.main_regressor.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9129eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ff214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(f\"Mean absolute error: {metrics.mean_absolute_error(y_test, preds)}\")\n",
    "print(f\"Mean squared error: {metrics.mean_squared_error(y_test, preds)}\")\n",
    "print(f\"R^2: {metrics.r2_score(y_test, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17c3ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.5733643988756195\n",
      "Mean squared error: 0.5429549244012856\n",
      "R^2: 0.6581879515557436\n"
     ]
    }
   ],
   "source": [
    "# Ridge with 500 components (or less)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(f\"Mean absolute error: {metrics.mean_absolute_error(y_test, preds)}\")\n",
    "print(f\"Mean squared error: {metrics.mean_squared_error(y_test, preds)}\")\n",
    "print(f\"R^2: {metrics.r2_score(y_test, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77b83903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6581879515557436"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec34579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15b99a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "\n",
    "#trans_test = mlb.fit_transform(test['genres'])\n",
    "#trans_columns = [f'genres_{column}' for column in mlb.classes_]\n",
    "#trans_df = pd.DataFrame(trans_test, columns=trans_columns)\n",
    "#result_df = pd.concat([test, trans_df], axis=1)\n",
    "#result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7699aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SeriesEncoder(X):\n",
    "    \n",
    "#     return pd.Series([{key: 1 for key in row} for row in X], name=X.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c35a4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# def DictEncoder(X):\n",
    "    \n",
    "#     if isinstance(X, pd.Series):\n",
    "#         print (f'DictEncoder treating X as a Series (name={X.name})')\n",
    "#         return SeriesEncoder(X)\n",
    "    \n",
    "#     else:\n",
    "    \n",
    "#         X_trans = pd.DataFrame()\n",
    "#         print (f'DictEncoder treating X as a DataFrame (columns={X.columns})')\n",
    "        \n",
    "#         for column in X.columns:\n",
    "# #             yield SeriesEncoder(X[column])\n",
    "\n",
    "#             encoded_column = SeriesEncoder(X[column])\n",
    "\n",
    "#             X_trans = pd.concat([X_trans, encoded_column], axis=1)\n",
    "\n",
    "#         return X_trans\n",
    "    \n",
    "#     return [{key: 1 for key in item} for item in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b1f65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ohe_list_column(series):\n",
    "    \n",
    "#     trans_series = mlb.fit_transform(series)\n",
    "    \n",
    "#     trans_columns_names = [f'{series.name}_{column}' for column in mlb.classes_]\n",
    "    \n",
    "#     trans_df = pd.DataFrame(trans_series, columns=trans_columns_names)\n",
    "    \n",
    "#     return trans_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
